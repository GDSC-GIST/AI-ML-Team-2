{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a83a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#강의 5 - 서로 다른 크기의 컬러 이미지를 데이터로 갖는 CNN\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#data input pipeline = imagedatagenerator로 구축\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#read files & read directory structures\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "logger = tf.get_logger()\n",
    "logger.setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f6bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.preprocessing.image.ImageDataGenerator\n",
    "# 디스크에서 데이터를 읽는 용도! -> Kaggle(URL)에서 데이터를 다운받고, \n",
    "# Colab(구글 클라우드) 로컬 디렉토리에 저장해야 함\n",
    "# !! Colab에서는 이렇게 클라우드 디렉토리에 데이터 받고 저장하지만, 일반적인 경우엔\n",
    "# read등으로 그냥 로컬 파일 불러오면 됨 !!\n",
    "# 구글 드라이브를 같이 쓰면 거기에서 불러올 수도 있음 (Colab)\n",
    "\n",
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "\n",
    "# _URL에서 데이터 다운받고 압축 풀어서 zip_dir에 저장함\n",
    "zip_dir = tf.keras.utils.get_file('cats_and_dogs_filterted.zip', origin=_URL, extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afacc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#zip_dir_base에, zip_dir의 디렉토리를 넣고, 로컬에서 어디에 있는지 프린트함\n",
    "# Colab에서는, 클라우드 상의 디렉토리가 출력됨\n",
    "\n",
    "zip_dir_base = os.path.dirname(zip_dir)\n",
    "!find $zip_dir_base -type d -print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c33b49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_dir은 각 데이터별(훈련, validation, 고냥이, 강아지별로 다른 디렉토리를 모두 저장함)\n",
    "# 그래서 이걸 데이터별로 분리하는 과정임\n",
    "\n",
    "base_dir = os.path.join(os.path.dirname(zip_dir), 'cats_and_dogs_filtered')\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')  # directory with our training cat pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')  # directory with our training dog pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')  # directory with our validation cat pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')  # directory with our validation dog pictures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a7729c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터별 디렉토리에 있는 파일 개수로, 훈련 및 validation 데이터셋의 데이터 개수를 셈\n",
    "\n",
    "num_cats_tr = len(os.listdir(train_cats_dir))\n",
    "num_dogs_tr = len(os.listdir(train_dogs_dir))\n",
    "\n",
    "num_cats_val = len(os.listdir(validation_cats_dir))\n",
    "num_dogs_val = len(os.listdir(validation_dogs_dir))\n",
    "\n",
    "total_train = num_cats_tr + num_dogs_tr\n",
    "total_val = num_cats_val + num_dogs_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b44fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training cat images:', num_cats_tr)\n",
    "print('total training dog images:', num_dogs_tr)\n",
    "\n",
    "print('total validation cat images:', num_cats_val)\n",
    "print('total validation dog images:', num_dogs_val)\n",
    "print(\"--\")\n",
    "print(\"Total training images:\", total_train)\n",
    "print(\"Total validation images:\", total_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54e1c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch정의, IMG크기 정의\n",
    "\n",
    "BATCH_SIZE = 100  # Number of training examples to process before updating our models variables\n",
    "IMG_SHAPE  = 150  # Our training data consists of images with width of 150 pixels and height of 150 pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2347802",
   "metadata": {},
   "source": [
    "Images must be formatted into appropriately pre-processed floating point tensors before being fed into the network. The steps involved in preparing these images are:\n",
    "\n",
    "1. Read images from the disk\n",
    "\n",
    "2. Decode contents of these images and convert it into proper grid format as per   their RGB content\n",
    "\n",
    "3. Convert them into floating point tensors\n",
    "\n",
    "4. Rescale the tensors from values between 0 and 255 to values between 0 and 1, as neural networks prefer to deal with small input values.\n",
    "\n",
    "Fortunately, all these tasks can be done using the class tf.keras.preprocessing.image.ImageDataGenerator.\n",
    "\n",
    ">> 이미지를 가져오고, 컬러 채널에 따라 3차원(이상)의 픽셀별 그리드를 만들고,\n",
    ">> 텐서로 바꾼 후, 픽셀별 0~255 값을 0~1로 정규화함\n",
    ">> 모든 걸 ImageDataGenerator로!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d803aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator 정의: 정규화 과정만 있음\n",
    "\n",
    "train_image_generator      = ImageDataGenerator(rescale=1./255)  # Generator for our training data\n",
    "validation_image_generator = ImageDataGenerator(rescale=1./255)  # Generator for our validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1983bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flow from directory -> Generator 이용함 : directory, 바꿀 이미지 크기를 정해야 함\n",
    "\n",
    "train_data_gen = train_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                                                           directory=train_dir,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150)\n",
    "                                                           class_mode='binary')\n",
    "\n",
    "val_data_gen = validation_image_generator.flow_from_directory(batch_size=BATCH_SIZE,\n",
    "                                                              directory=validation_dir,\n",
    "                                                              shuffle=False,\n",
    "                                                              target_size=(IMG_SHAPE,IMG_SHAPE), #(150,150)\n",
    "                                                              class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b54fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# matplotlib으로 훈련 데이터 그래프 그리기\n",
    "\n",
    "#next = traindatagen에서 한 개의 batch를 가져옴 (batch = image, label로 구성된 튜플)\n",
    "sample_training_images, _ = next(train_data_gen) \n",
    "\n",
    "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "plotImages(sample_training_images[:5])  # Plot images 0-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc210267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 제작하기\n",
    "# Conv2D + MaxPooling 4회, 그리고 Flatten -> 512unit의 인풋 Dense\n",
    "# Output은 softmax쓰고, 라벨 2개인 Dense\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    \n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', \n",
    "                           input_shape = (150, 150, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation = 'relu'),\n",
    "    tf.keras.layers.Dense(2)\n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b2e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 Compilation\n",
    "#optimizer = adam, loss = softmax를 사용하므로 SparseCategoricalCrossentropy 사용함\n",
    "#metric = 각 epoch마다 training set accuracy, 그리고 validation set accuracy를 같이 나타내기 위해 사용함\n",
    "\n",
    "model.compile(optimizer = 'adam',\n",
    "             loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#우리가 제작한 모델 layer별로 parameter 개수까지 한번에 나타내기\n",
    "# model.summary()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778a37d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#모델 Training\n",
    "\n",
    "#model.fit_generator : 우리가 ImageDataGenerator를 쓰고 있기 때문에, 이걸 씀\n",
    "# validation_data = (이름) : 훈련 데이터 말고도 validation dataset도 쓰고 있으므로, 훈련할 때 같이 기입함\n",
    "\n",
    "#이전과는 다르게, 훈련 이전 모델 = model, 훈련 이후 모델 = history로 객체 정의함\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "history = model.fit_generator(\n",
    "        \n",
    "        train_data_gen,\n",
    "        steps_per_epoch = int(np.ceil(total_train/ float(BATCH_SIZE))),\n",
    "        \n",
    "        epochs = EPOCHS,\n",
    "        \n",
    "        validation_data = val_data_gen,\n",
    "        validation_steps = int(np.ceil(total_val / float(BATCH_SIZE)))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c9f056",
   "metadata": {},
   "source": [
    "이대로 훈련하게 되면,\n",
    "1. training data에 대한 정확도 = 100%\n",
    "2. validation data에 대한 정확도 = 75% 가 나오게 됨\n",
    ">> 모델이 훈련 데이터를 외우고 있음! 즉, 실제로 분류하는데 필요한 internal variable이 아니라, 훈련 데이터만을 분류할 수 있도록 (개, 고양이)의 특징이 아니라 훈련 데이터 사진만의 특징까지 넣어 optimizer가 작동하고 있음.\n",
    "\n",
    "= overfitting이라고 부름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47617940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#그래프 그리기\n",
    "\n",
    "# 훈련한 모델의 데이터 일부 불러오기\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "\n",
    "# 현재 figure크기 8x8\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# 1행 2열로 2개 그래프 분리, 1번 그래프 다룸\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "#x값, y값, 라벨\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "\n",
    "#한번 더 쓰면, 위 코드랑 아래 코드 그래프가 동시에 하나에 그려진다는 거\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "\n",
    "#그냥 라벨을 따로 모아서 보여주는 거\n",
    "plt.legend(loc='lower right')\n",
    "#제목\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "#오른쪽 그래프(2번 그래프)를 다룬다는 뜻!\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig('./foo.png')\n",
    "\n",
    "#그래프 화면에 띄우기\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477e9ef",
   "metadata": {},
   "source": [
    "** 참고\n",
    "https://soooprmx.com/matplotlib%EC%9D%98-%EA%B8%B0%EB%B3%B8-%EC%82%AC%EC%9A%A9%EB%B2%95-%EB%B0%8F-%EB%8B%A4%EB%A5%B8-%EC%8B%9C%EA%B0%81%ED%99%94-%EB%9D%BC%EC%9D%B4%EB%B8%8C%EB%9F%AC%EB%A6%AC/\n",
    "matplotlib에서,\n",
    "\n",
    "figure = 그래프를 그리는 캔버스 역할\n",
    "plot = 차트(그래프)\n",
    "axes = 그래프를 부르는 다른 말! subplots()를 쓸 때 주로 등장하게 됨\n",
    "axis = x, y 범위의 제한값\n",
    "\n",
    "\n",
    "plt.() = 현재 figure에 그래프를 그리겠다는 뜻\n",
    "plt.plot(x축에 넣을 배열, y축에 넣을 배열, 그래프 라벨)\n",
    "plt.subplot(행, 열, 인덱스) --> plt.subplot(1, 2, 1)은, 한 figure 안에 그래프를 1행, 2열로 그리고(즉, 그래프 두 개를 한 화면에 그리고), 그 중에서 1번 인덱스 그래프(즉, 왼쪽 그래프)를 다루고 있다는 의미. plt.subplot(1,2,1)다음에 나오는 plt.plot은 모두 왼쪽 그래프에만 해당되는 코드임.\n",
    "\n",
    "A, B = plt.subplots(행, 열, sharex=True)\n",
    ">> plt.subplots는 figure, axes의 쌍을 반환함! 여기서 sharex는 위랑 아래에 있는 그래프가 x축 공유한다는 뜻\n",
    "A, B = plt.subplots(2, 2) 이렇게 쓰면, 각 그래프를 다룬다는 뜻으로\n",
    "B[0,0].plot(x값, y값) --> 2행 2열 중 왼쪽 위 그래프에 선 그래프를 그림\n",
    "B[1, 1].plot --> 2행 2열 중 오른쪽 아래 그래프"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5577834e",
   "metadata": {},
   "source": [
    "그래프를 그려 보면,\n",
    "\n",
    "10개 정도의 epoch후에 validation accuracy는 일정히 유지되고, 오히려 validation dataset에 대한 loss는 증가함\n",
    "하지만 훈련 데이터에 대한 정확도는 1에 가까워지고, loss는 0에 가까워짐\n",
    "\n",
    ">> overfitting!!\n",
    "\n",
    "해결법?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
