{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "l10c03_nlp_constructing_text_generation_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "6d37fb8d-b22f-434d-d7f4-1e3b47b724e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-20 12:08:55--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 173.194.193.102, 173.194.193.138, 173.194.193.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|173.194.193.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4lhmoubaqc6ngs7sf6jp53h72ookc500/1637410125000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-11-20 12:08:57--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4lhmoubaqc6ngs7sf6jp53h72ookc500/1637410125000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 64.233.191.132, 2607:f8b0:4001:c0c::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|64.233.191.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   175MB/s    in 0.4s    \n",
            "\n",
            "2021-11-20 12:08:57 (175 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation - 띄어쓰기를 모두 삭제함 (\" \" -> \"\")\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line \n",
        "  # dataset을 1개로 만든 후\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  # 한 줄씩 띄어 나눔\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "df362aac-7801-4f34-d70d-2d8f322e8173",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  if sys.path[0] == '':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "86d786e7-d30d-4cdf-b836-aa2e01445cb9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "# 여기선, one-hot output 배열의 전체 길이는 토큰의 개수와 같음\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "a3bc8cd0-1e7f-4168-f1cd-3671d686e836",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "#Dense output layer는 이젠 1이 아니라, 반환하고 싶은 단어의 개수만큼 유닛을 가짐\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=200, verbose=1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 10s 18ms/step - loss: 5.9826 - accuracy: 0.0277\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 5.4498 - accuracy: 0.0383\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 5.3685 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 5.3152 - accuracy: 0.0434\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 5.2445 - accuracy: 0.0429\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 5.1745 - accuracy: 0.0409\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 5.1119 - accuracy: 0.0429\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 5.0547 - accuracy: 0.0489\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.9939 - accuracy: 0.0570\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.9359 - accuracy: 0.0671\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.8586 - accuracy: 0.0737\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.7872 - accuracy: 0.0762\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.7050 - accuracy: 0.0812\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.6279 - accuracy: 0.0893\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.5453 - accuracy: 0.0984\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.4626 - accuracy: 0.0999\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.3778 - accuracy: 0.1206\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 4.2900 - accuracy: 0.1241\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.2090 - accuracy: 0.1423\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.1271 - accuracy: 0.1554\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 4.0445 - accuracy: 0.1645\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.9586 - accuracy: 0.1887\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.8826 - accuracy: 0.1968\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.8100 - accuracy: 0.2089\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.7267 - accuracy: 0.2235\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.6548 - accuracy: 0.2452\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.5825 - accuracy: 0.2684\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.5077 - accuracy: 0.2861\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.4338 - accuracy: 0.3017\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 3.3716 - accuracy: 0.3128\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.3114 - accuracy: 0.3335\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.2400 - accuracy: 0.3436\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.1826 - accuracy: 0.3572\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.1215 - accuracy: 0.3613\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.0596 - accuracy: 0.3759\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 3.0134 - accuracy: 0.3905\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.9445 - accuracy: 0.3986\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.8829 - accuracy: 0.4198\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.8212 - accuracy: 0.4092\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 2.7723 - accuracy: 0.4364\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 2.7123 - accuracy: 0.4541\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 2.6607 - accuracy: 0.4596\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 2.6123 - accuracy: 0.4783\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.5506 - accuracy: 0.4884\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.5080 - accuracy: 0.4955\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.4687 - accuracy: 0.5081\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.4214 - accuracy: 0.5166\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.3761 - accuracy: 0.5242\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.3300 - accuracy: 0.5333\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.2865 - accuracy: 0.5469\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 2.2468 - accuracy: 0.5499\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.2050 - accuracy: 0.5600\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.1542 - accuracy: 0.5686\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 2.1122 - accuracy: 0.5792\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 2.0716 - accuracy: 0.5838\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 2.0408 - accuracy: 0.5822\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 2.0091 - accuracy: 0.5863\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.9786 - accuracy: 0.6004\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.9693 - accuracy: 0.6100\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.9214 - accuracy: 0.6070\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.8814 - accuracy: 0.6327\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.8523 - accuracy: 0.6322\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.8101 - accuracy: 0.6418\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.7753 - accuracy: 0.6463\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.7625 - accuracy: 0.6468\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.7428 - accuracy: 0.6559\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.7097 - accuracy: 0.6625\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.6752 - accuracy: 0.6670\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.6312 - accuracy: 0.6771\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.5980 - accuracy: 0.6867\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.5897 - accuracy: 0.6862\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.5574 - accuracy: 0.7008\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.5502 - accuracy: 0.6942\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.5294 - accuracy: 0.6963\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.4857 - accuracy: 0.7053\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.4587 - accuracy: 0.7074\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.4321 - accuracy: 0.7164\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.4090 - accuracy: 0.7200\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.3830 - accuracy: 0.7250\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.3754 - accuracy: 0.7306\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.3627 - accuracy: 0.7306\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.3459 - accuracy: 0.7397\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.3130 - accuracy: 0.7422\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.2945 - accuracy: 0.7442\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.2657 - accuracy: 0.7497\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.2759 - accuracy: 0.7422\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.2528 - accuracy: 0.7497\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.2351 - accuracy: 0.7513\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.2295 - accuracy: 0.7533\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.2124 - accuracy: 0.7558\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.2413 - accuracy: 0.7457\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.1843 - accuracy: 0.7608\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.1713 - accuracy: 0.7603\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.1416 - accuracy: 0.7634\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.1179 - accuracy: 0.7760\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.0988 - accuracy: 0.7775\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.0780 - accuracy: 0.7785\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.0606 - accuracy: 0.7841\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.0714 - accuracy: 0.7856\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.1038 - accuracy: 0.7689\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.0638 - accuracy: 0.7815\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.0352 - accuracy: 0.7851\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.0096 - accuracy: 0.7967\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.0147 - accuracy: 0.7936\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.0127 - accuracy: 0.7846\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 1.0298 - accuracy: 0.7846\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 1.0086 - accuracy: 0.7911\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.9789 - accuracy: 0.7972\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.9526 - accuracy: 0.8042\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.9375 - accuracy: 0.8078\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.9164 - accuracy: 0.8073\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.8957 - accuracy: 0.8189\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.8948 - accuracy: 0.8143\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.8749 - accuracy: 0.8214\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.8643 - accuracy: 0.8244\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.8474 - accuracy: 0.8239\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.8399 - accuracy: 0.8249\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.8257 - accuracy: 0.8264\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.8160 - accuracy: 0.8305\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.8091 - accuracy: 0.8290\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.7976 - accuracy: 0.8355\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.7987 - accuracy: 0.8280\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.8447 - accuracy: 0.8239\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.8190 - accuracy: 0.8239\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.7763 - accuracy: 0.8345\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.7621 - accuracy: 0.8385\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.7477 - accuracy: 0.8401\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.7344 - accuracy: 0.8421\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.7320 - accuracy: 0.8446\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.7434 - accuracy: 0.8396\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.7240 - accuracy: 0.8481\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.7075 - accuracy: 0.8421\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.6993 - accuracy: 0.8502\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.6894 - accuracy: 0.8491\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.6836 - accuracy: 0.8542\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.6750 - accuracy: 0.8537\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.6700 - accuracy: 0.8542\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.6713 - accuracy: 0.8542\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.6623 - accuracy: 0.8537\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.6516 - accuracy: 0.8562\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.6490 - accuracy: 0.8577\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.6387 - accuracy: 0.8572\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.6280 - accuracy: 0.8633\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.6205 - accuracy: 0.8613\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.6164 - accuracy: 0.8613\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.6101 - accuracy: 0.8638\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.6063 - accuracy: 0.8618\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.5959 - accuracy: 0.8613\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5866 - accuracy: 0.8688\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5820 - accuracy: 0.8678\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5783 - accuracy: 0.8688\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5742 - accuracy: 0.8638\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.5755 - accuracy: 0.8668\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5733 - accuracy: 0.8693\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5755 - accuracy: 0.8653\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.5851 - accuracy: 0.8602\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5695 - accuracy: 0.8648\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.5629 - accuracy: 0.8658\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5799 - accuracy: 0.8587\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5817 - accuracy: 0.8587\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.6079 - accuracy: 0.8537\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.5791 - accuracy: 0.8628\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5437 - accuracy: 0.8759\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.5297 - accuracy: 0.8739\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5279 - accuracy: 0.8764\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.5236 - accuracy: 0.8703\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5097 - accuracy: 0.8739\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.5037 - accuracy: 0.8754\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4975 - accuracy: 0.8774\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4932 - accuracy: 0.8789\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.4879 - accuracy: 0.8835\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4867 - accuracy: 0.8799\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.4828 - accuracy: 0.8804\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.4772 - accuracy: 0.8804\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.4761 - accuracy: 0.8829\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.4738 - accuracy: 0.8829\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4675 - accuracy: 0.8845\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.4610 - accuracy: 0.8794\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4645 - accuracy: 0.8814\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4712 - accuracy: 0.8845\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.4828 - accuracy: 0.8804\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4835 - accuracy: 0.8713\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4753 - accuracy: 0.8789\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4602 - accuracy: 0.8789\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4525 - accuracy: 0.8855\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4438 - accuracy: 0.8855\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 1s 20ms/step - loss: 0.4378 - accuracy: 0.8885\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4433 - accuracy: 0.8865\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4365 - accuracy: 0.8845\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4437 - accuracy: 0.8865\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4483 - accuracy: 0.8840\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.4275 - accuracy: 0.8935\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.4213 - accuracy: 0.8910\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.4229 - accuracy: 0.8905\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4202 - accuracy: 0.8870\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4135 - accuracy: 0.8925\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.4023 - accuracy: 0.8905\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.3991 - accuracy: 0.8920\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 1s 18ms/step - loss: 0.3958 - accuracy: 0.8930\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 0.3988 - accuracy: 0.8930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "4175392b-ca8d-4044-8853-6bd0c5b1451e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "569897d0-43fd-4c48-e36f-01624ac7760e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "im feeling chills me again i had to your father before end but my meant to never kisses girl sender at start brother touch get brother touch get bed brother touch bed brother touch heel intention brother touch bed brother touch cryin men slowly again me down sailing end me mad sailing will end the selfish here but here again sailing me mad past feel girl mad past stuff fine fine bedumbedumdum found out sees me girl down girl mad girl mad mad feel stuff stuff stuff been past been over past warm world found did learn touch touch burying meant to but\n"
          ]
        }
      ]
    }
  ]
}